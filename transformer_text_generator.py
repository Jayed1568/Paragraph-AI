# -*- coding: utf-8 -*-
"""Transformer_Text_Generator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RkJLj71zVSna_zHAqByDjstl5ubvJuhB

**Setup and Install Required Libraries**
"""

!pip install pandas transformers datasets

!pip install --upgrade transformers datasets

"""**Upload and Load the Dataset**"""

# Read the uploaded .txt file
with open("/content/ParagraphAI/Paragraphs.txt", "r", encoding="utf-8") as f:
    data = f.read()

# Split paragraphs by "---"
samples = [s.strip() for s in data.split('---') if s.strip()]

"""**Convert to Prompt/Completion Format**"""

dataset = []

for sample in samples:
    lines = sample.split("\n")
    if not lines or len(lines) < 2:
        continue
    prompt = lines[0].strip()
    completion = " ".join(line.strip() for line in lines[1:] if line.strip())
    dataset.append({"prompt": prompt, "completion": completion})

"""**Create Dataset for Training**"""

import pandas as pd
df = pd.DataFrame(dataset)
df.head()

df.to_csv("./prompt_completion_dataset.csv", index=False)

"""**Tokenization and Dataset Preparation**"""

from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
from datasets import Dataset

tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

# Modified tokenize function to handle batched input
def tokenize(examples):
    # examples is a dictionary where values are lists when batched=True
    prompts = examples['prompt']
    completions = examples['completion']

    # Create a list of formatted input texts for the batch
    input_texts = [f"### PROMPT:\n{p}\n\n### COMPLETION:\n{c}{tokenizer.eos_token}" for p, c in zip(prompts, completions)]

    # Tokenize the list of input texts
    return tokenizer(input_texts, truncation=True, padding='max_length', max_length=512)

hf_dataset = Dataset.from_pandas(df)
# Use batched=True as intended, with the updated tokenize function
tokenized_dataset = hf_dataset.map(tokenize, batched=True)

# Add the 'labels' column which is a copy of 'input_ids'
tokenized_dataset = tokenized_dataset.add_column("labels", tokenized_dataset["input_ids"])

"""**Model Preparation and Fine-tuning**"""

from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
import os
os.environ["WANDB_DISABLED"] = "true"

model = AutoModelForCausalLM.from_pretrained("openai-community/gpt2")

training_args = TrainingArguments(
    output_dir="./ParagraphAI/results",
    per_device_train_batch_size=2,
    num_train_epochs=3,
    logging_dir='./logs',
    logging_steps=10,
    save_steps=500,
    save_total_limit=1,
    fp16=True,
    warmup_steps=10,
    # Add these lines for better logging of the loss
    report_to=["none"], # Disable reporting to external services if not needed
    logging_first_step=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset
)

# trainer.train() should work as the dataset has the 'labels' column
trainer.train()

"""**Inference (Generating Text)**"""

def generate_text(prompt, max_length=100):
    input_text = f"### PROMPT:\n{prompt}\n\n### COMPLETION:\n"
    inputs = tokenizer.encode(input_text, return_tensors="pt").cuda()
    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

print(generate_text(prompt="How the traffic jam is Harming us?"))

"""Exporting the Model"""

model.save_pretrained("./ParagraphAI/my_finetuned_gpt2")
tokenizer.save_pretrained("./ParagraphAI/my_finetuned_gpt2")

"""**Exporting The Logs**"""

import pandas as pd

log_history = trainer.state.log_history
df = pd.DataFrame(log_history)
df.to_csv("./ParagraphAI/training_log.csv", index=False)

"""**Zipping the whole File**"""

import shutil

shutil.make_archive("/content/ParagraphAI", 'zip', ".")

"""Done By WAHIB UL MALIK"""